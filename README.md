# ðŸš€ ConfliRAG â€” Detecting LLM Behavior Under Conflicting Evidence

## ðŸŽ¯ Core Goal

Build a RAG-based evaluation system that intentionally retrieves contradictory fictional documents and analyzes how LLM outputs behave under conflict:

- Do they merge inconsistent facts?
- Do they pick a side?
- Do they resolve the conflict transparently?
- Do they hallucinate explanations?
- Do they hide contradictions?

And build a **Transparency Classifier** that scores whether the model handled conflict honestly.
